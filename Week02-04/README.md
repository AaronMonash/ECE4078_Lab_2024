# Milestone 2: Simultaneous Localization And Mapping (SLAM)
- [Introduction](#Introduction)
    - [ARUCO markers](#ARUCO-markers)
    - [SLAM helper scripts](#SLAM-helper-scripts)
- [Activities](#Activities)
    - [Calibration (week 2)](#Calibration-week-2)
    - [SLAM (week 3-4)](#SLAM-week-3-4)
- [M2 Marking](#M2-Marking)
    - [Evaluation scheme](#evaluation-scheme)
    - [Instructions](#marking-instructions)
- [Technical FAQs](#FAQs-M2)

---
## Introduction
Imagine your robot arrives at a new supermarket which has an unknown layout. It can read aisle labels with its camera and remembers how it has moved since it started at location (0,0,0) (x,y,theta). How can it figure out where things are and where itself is positioned at a given time?

In M2, the robot will perform Simultaneous Localization And Mapping (SLAM) in a 3mx3m arena (supermarket), which contains 10 ARUCO markers (aisle labels). The robot will always start at the centre of the map, i.e., (x,y,theta) = (0,0,0). 

You will use your M1 (teleoperation) code to drive your robot around this unknown supermarket strategically, so that it can estimate where things are (creating a map of the arena with marker coordinates) and where itself is at any given time using its camera and motion model.

**You'll need to install additional python packages** for M2 by typing the following commands in the terminal (remember to activate your venv first by typing ```PenguinPi\Scripts\activate```):
```
python3 -m pip install machinevision-toolbox-python spatialmath-python==0.8.9 opencv-contrib-python==4.1.2.30 matplotlib
```

If you are using the Linux env (sim + physical robot), after installing the python packages, run the following commands to update your catkin_ws
```
cd ~/catkin_ws/
source ~/catkin_ws/devel/setup.bash
catkin_make
```

To confirm the required packages are installed, in a terminal type ```python3``` (with the venv activated), which opens python 3 in command line, then type:
```
import cv2
from cv2 import aruco
from machinevisiontoolbox import Image, CentralCamera
```
If there is no error then everything is successfully installed and you can exit the command line python 3 by typing ```exit()```

### ARUCO markers
[ARUCO markers](http://www.uco.es/investiga/grupos/ava/node/26) are square fiducial markers introduced by Rafael Mu√±oz and Sergio Garrido. OpenCV contains a trained [function](https://docs.opencv.org/trunk/d5/dae/tutorial_aruco_detection.html) that detects the ARUCO markers, which will be used in this project (the dictionary we used to generate the markers was ```cv2.aruco.DICT_4X4_100```). PenguinPi will be using these ARUCO markers as aisle labels to help it map the environment and locate itself. 

Note: You can make your own ARUCO marker blocks for development outside of the labs using [the print file provided](diy_prints/LabPrintingMarker_A3_Compact.pdf) (or [the foldable version](diy_prints/LabPrintingMarker_A3.pdf)). The marker size used in this lab is **7cm** (black square area) as specified in [aruco_detector.py](slam/aruco_detector.py#L10). Please print the pdf using **A3-size papers** and beware of printing rescaling, as inaccurate size of the marker will lead to inaccurate estimations.

### SLAM helper scripts
The following helper scripts are provided to support your development of SLAM
- [operate.py](operate.py) is the central control program that combines both keyboard teleoperation (M1) and SLAM. It's an extension from the operate.py script in M1. Running this file requires the [utility scripts](../Week00-01/util) and the [GUI pics](../Week00-01/pics) from M1, and the [calibration parameters](calibration/param/) and [SLAM scripts](slam/) of M2. After the GUI is launched, you can start or stop SLAM by pressing ENTER, and save a map generated by SLAM by pressing "s". It provides a visualization of the robot's view with overlay of identified ARUCO markers, the SLAM visualization, and a 5-minute countdown clock (feel free to change the countdown or remove it. it's just provided as a reminder of the [overall marking slot time limit](M2_marking_instructions.md)). Note: remember to replace the [keyboard control section](operate.py#L200) with your teleoperation codes from M1.
- [Calibration scripts](calibration/) and [default parameters](calibration/param/) for wheel and camera calibration: see [calibration section](#Calibration-week-2) for details
- [Designs](diy_prints/) of the ARUCO marker blocks and the camera calibration rig for setting up your own development arena outside of the labs
- [aruco_detector.py](slam/aruco_detector.py) uses OpenCV to detect ARUCO markers and provides an estimation to their positions, which will be used for SLAM in addition to the drive signals
- [mapping_utils.py](slam/mapping_utils.py) saves the SLAM map for evaluation
- [SLAM_eval.py](SLAM_eval.py) evaluates a SLAM map against the ground truth
- [TrueMap.txt](TrueMap.txt) is an example map for setting up an arena and for evaluating your SLAM during development. This map will NOT be used for marking.

### Map generator
To help you generate ground-truth maps for evaluating your SLAM implementation in a physical arena, we provide a [map generator](../image_to_map_generator), which creates maps from an overview image of the arena. It's provided as-is and for reference only. Don't use it to cheat.

---
## Activities
**Note:** Below are instructions for developing SLAM using the physical robot. For development using the simulation environment please see [these M2 sim instructions](M2_sim.md). The simulator can be useful for precise control of robot and object poses, as well as easy spawning of random maps. However, be aware of the "Sim-to-Real gap" when developing in the simulator, which is a common limit in robotics.

### Calibration (week 2)

#### Step 1) Preparing Working directory
Prior to working on this week's materials, please make sure you do the following:
- Copy over the [util folder](../Week00-01/util) and the [GUI pics](../Week00-01/pics) from the M1 lab into your working directory
- Replace the [keyboard control section](operate.py#L200) in operate.py with the code you developed for M1

#### Step 2) Wheel calibration
Completed [wheel_calibration.py](calibration/wheel_calibration.py) by filling in the required lines of code (computing the [scale parameter](calibration/wheel_calibration.py#L46) and the [baseline parameter](calibration/wheel_calibration.py#L89)), run the [wheel calibration script](calibration/wheel_calibration.py) using the command ```python3 wheel_calibration.py --ip 192.168.50.1 --port 8080```. This script will set the robot driving forward, and then spinning at various velocities, for durations that you specify, in order to compute the scale and baseline parameters for wheel calibration.

You can mark a 1m long straight line with masking tape on the floor, and use it as a guide to check if the robot has travelled exactly (as close as possible) 1m. Masking tape and measuring tape will be provided to you in the lab. 

#### Step 3) Camera calibration
Complete [calib_pic.py](calibration/calib_pic.py) using your M1 teleoperation codes and run the completed script using ```python3 calib_pic.py --ip 192.168.50.1 --port 8080```, then press ENTER to take a calibration photo (It will be saved as [calib_0.png](calibration/calib_0.png)). You have to drive the robot fairly close to the calibration rig to get a good view of the 8 dots. The photo should look something like this:

![Real calibration photo](screenshots/RealCameraCalib.png?raw=true "Real calibration photo")

Once you have taken the [calib_0.png](calibration/calib_0.png) photo with the physical robot, run ```python3 camera_calibration.py``` to perform camera calibration. This opens the calibration photo you just took. Selecting the 8 key points in the calibration photo following the ordering shown in [calibration-fixture.png](diy_prints/calibration-fixture.png) by left clicking on each point (right click to cancel a selected point). Once all 8 points are selected, close the figure window to compute the camera matrix. This will update the [intrinsic parameters](calibration/param/intrinsic.txt). Note: keep the [distortion coefficients](calibration/param/distCoeffs.txt) to all 0s.

![Calibration fixture](diy_prints/calibration-fixture.png?raw=true "Calibration fixture")

Note: You can make your own calibration rig for development outside of the labs using [the print file provided](diy_prints/LabPrintingRig_A3.pdf) (notice the small square in the top left corner in addition to the dots). The size of the rig is specified in [calibration-fixture.png](diy_prints/calibration-fixture.png). Please print the pdf using **A3-size papers** and beware of printing rescaling. Make sure that the panels are at 90deg to each other and the distance between the calibration dots are as specified when making the rig, as this will influence the accuracy of your estimations. We will bring a calibration rig to the labs for you to perform the calibration during the lab sessions too.

---
### SLAM (week 3-4)
[operate.py](operate.py) makes use of the camera and wheels' [calibrated parameters](calibration/param) and the [SLAM](slam/) components to produce a map saved as ```slam.txt``` in the ```lab_output``` folder. This SLAM map contains a list of identified ARUCO makers, their locations and the covariances of the estimation. Note: remember to replace the [keyboard control section](operate.py#L200) with your codes from M1.

[SLAM](slam/ekf.py) computes the locations of the ARUCO markers using both [camera based estimation](slam/aruco_detector.py) and [motion model based estimation](slam/robot.py).

**Please complete [robot.py](slam/robot.py) by filling in the computation of the [derivatives](slam/robot.py#L79) and [covariance](slam/robot.py#L127) of the motion model. Please also complete [ekf.py](slam/ekf.py) by filling in the computation of the [predicted robot state](slam/ekf.py#L93) and the [updated robot state](slam/ekf.py#L117) to finish the extended Kalman filter function.**

Once robot.py and ekf.py are completed, you can test the performance of your SLAM by running ```python3 operate.py --ip 192.168.50.1 --port 8080```

Below are examples of what the GUI running SLAM looks like on physical robot and in sim:

![Example SLAM visualization on physical robot](screenshots/SLAMvis.png?raw=true "Example SLAM visualization in Gazebo")

![Example SLAM visualization in Gazebo](screenshots/GazeboSLAMvis.png?raw=true "Example SLAM visualization in Gazebo")

#### Evaluating the SLAM performance
An evaluation script [SLAM_eval.py](SLAM_eval.py) is provided for evaluating the map generated by SLAM against the true map.

A [development map](TrueMap.txt) is provided which you can use for setting up the physical/simulator arena and for evaluation. It is the same as "map1.txt" provided when you were setting up the simulator env in [week 1](../Robot_simulator/InstallationGuideSim.md).

Run ```python3 SLAM_eval.py TrueMap.txt lab_output/slam.txt``` and you should see a printout of the evaluation results:
 
![Example output of SLAM evaluation script](screenshots/SLAM_eval_output.png?raw=true "Example output of SLAM evaluation script")

---
## M2 Marking
### Evaluation scheme
To allow for the best performance of your SLAM module, you will be marked based on finding the 10 ARUCO markers, *the RMSE after alignment* between your estimations and the true locations of these markers during a live demonstration conducted in a **NEW MAP** in week 5. After the live demo, the map generated will be marked against the ground-truth map using [SLAM_eval.py](SLAM_eval.py). 5pt will be deducted for each marker the robot has collided into during the live demo. Your M2 mark is computed as:

slam_score = ((0.12 - Aligned_RMSE)/(0.12 - 0.02)) x 80

**Total M2 mark = slam_score + (NumberOfFoundMarkers x 2) - (NumberOfCollidedMarkers x 5)**


**Note:** If your Aligned_RMSE value goes above the upper bound 0.12, your slam_score will be 0. If the Aligned_RMSE value goes below the lower bound 0.02, your slam_score will be 80, i.e., 0 ‚â§ slam_score ‚â§ 80

In the example above, if no collision happened during the live demo, as all 10 markers are found and Aligned_RMSE = 0.075, slam_score = ((0.12 - 0.075)/(0.12 - 0.02)) x 80 = 36, the total M2 mark will be 56 (36+20-0) out of 100pt.

### Marking instructions
Please see [M2 marking instructions](M2_marking_instructions.md)

---
## FAQs: M2
- If you are using Mac to run the VM and are encountering performance issues, please follow the steps in [this link](https://www.reddit.com/r/virtualbox/comments/houi9k/how_to_fix_virtualbox_61_running_slow_on_mac/) 
- Refer to the comments in each of the [calibrateWheelRadius](calibration/wheel_calibration.py#L10) and [calibrateBaseline](calibration/wheel_calibration.py#L52) functions for that each of these values corresponds to on the physical robot
- Take a close look at the units of the expected output when formulating your calculations. Referring to these equations may be helpful:

![Useful equations for calculating baseline](screenshots/AngularVelocity.png?raw=true "Useful equations for calculating baseline")

- It is recommended that you keep the file structure for this lab material (and future weeks) unchanged to avoid path errors
- Remember to reach out via Slack if you encounter issues between lab sessions
- Beware of the sign error that can happen with calculating the difference between the measurement and estimate in the correction step
- The state vector x will be appended with the aruco marker measurements. Take a note of the location of x that should be updated in the motion model.
- In the prediction step, we should update the mean belief by driving the robot.
- If you encounter error while trying to install ```opencv-contrib-python==4.1.2.30```, install the newest version of opencv-contrib-python by not specifying the version.
- If you cannot run ```from machinevisiontoolbox import Image, CentralCamera``` due to spatialmath error, uninstall the current version of spatialmath-python and re-install with the newest one.
- [calib_pic.py](calibration/calib_pic.py) and [camera_calibration.py](calibration/camera_calibration.py) have been updated to work with the 2 changes above and the newest version of machine vision toolbox, please check if you have the correct code.
- If you are using an older version of the machinevisiontoolbox, you can use the previous camera calibration script [camera_calibration_old.py](calibration/camera_calibration_old.py), or update your MVT by reinstalling it
- If you are using an older version of OpenCV (using the VM image or installing with the version flag), please use the previous aruco detector script [aruco_detector_old.py](slam/aruco_detector_old.py). If you are using the new version of OpenCV (installing without a specified version flag), please update your aruco detector script [aruco_detector.py](slam/aruco_detector.py) (Line 15 and 16 have been changed compared to the old script)
- In the predict function which you need to fill out in [ekf.py](slam/ekf.py), the comment originally stated that you should compute the predicted x, whereas you should actually be computing P, the predicted state. This comment has now been adjusted.
