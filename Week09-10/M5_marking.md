# Milestone 5: Integrated System (trial run for final demo) marking instructions
- [Marking schemes](#marking-schemes)
    - [Evaluation](#evaluation)
    - [Rules](#rules)
- [Marking steps](#marking-steps)
- [Marking checklist](#marking-checklist)

The final demo is likely to be the same as M5, although minor details/rules/marking scheme are still subject to change. **PLEASE MAKE SURE YOU READ THE MARKING INSTRUCTIONS CAREFULLY.**

---
## Marking schemes
---
### Evaluation
We have divided M5 into two components:
1. Arena mapping (60pts)
2. Grocery shopping (40pts)


#### 1. Arena mapping (60pts)
Before the navigation task, you may choose to manually drive the robot around first and create a map of the 10 ArUco markers as ```slam.txt``` ([M2: SLAM](../Week02-04/)) and a map of the 10 objects as ```targets.txt``` ([M3: CV](../Week05-06/)), or you can generate the SLAM and object maps during the autonomous navigation. 

Either way, we will evaluate your ```slam.txt``` and ```targets.txt``` for arena mapping. Please make sure that your generated maps follow the same format of the [example SLAM](../Week05-06/lab_output/slam.txt) and [target map](../Week05-06/lab_output/targets.txt) and pair them by including attempt number IDs when renaming the maps as "slam_{attempt_no}.txt" and "targets_{attempt_no}.txt" (see Step 3 of the [Marking Steps](#marking-steps)).

Your SLAM and object maps will be evaluated using [mapping_eval.py](../Week05-06/mapping_eval.py). If your estimation maps are not in the expected format and cannot be compared against a groundtruth map ([example groundtruth map](../Week07-08/M4_prac_map_full.txt)) by the evaluation script you will receive 0pts for arena mapping.

The mark calculation will be adjusted according to changes in the total arena mapping scores. We recommend you change [line 301](../Week05-06/mapping_eval.py#L286) to
```
slam_score = ((0.12 - slam_rmse_aligned)/(0.12 - 0.02)) * 24 + len(taglist) * 0.6
target_est_score = np.sum(target_scores) * 0.375
print(f'SLAM Score (0 to 30): = {np.round(slam_score, 3)}')
print(f'Target Score (0 to 30): = {np.round(target_est_score, 3)}')
```
to evaluate your own mapping scores.
##### 1.1 SLAM (30pts)

```slam.txt``` should contain the poses of 10 ArUco markers in the arena. Similar to M2, the SLAM score will be calculated based on the Aligned_RMSE, using

~~~
slam_score = (0.12 - Aligned_RMSE)/(0.12 - 0.02) x 24 + NumberOfFoundMarkers x 0.6
0 ≤ slam_score ≤ 30
~~~

[NOTE] If you miss markers while mapping, the indiviual error of those markers will be assumed to be the highest error out of the detected markers. For example, your detected markers 1 to 9, and marker 4 had the largest error of 10 cm after alignment, this means that the error of marker 10 is assumed to be 10 cm as well.

##### 1.2 Target pose estimation (30pts)

```targets.txt``` should contain the poses of 10 fruits&vegs in the arena. Similar to M3, the target score for each object will be calculated based on the pose estimate error, using

~~~
target_score[object] = (1 - estimation_error[object])/(1-0.025) x 3
target_est_score = sum(target_score)
0 ≤ target_est_score ≤ 30
~~~

##### 1.3 Mapping total (60pts)

After the individual scores of SLAM and target pose estimation are calculated, your final arena mapping score will be

~~~
mapping_score = slam_score + target_est_score
0 ≤ mapping_score ≤ 60
~~~

#### 2. Grocery shopping (40pts)
Using the maps generated by your SLAM and CV components, your robot will perform a navigation task to reach targets on the provided shopping list. The marking criteria will be similar to M4. The concept of a [qualified run](../Week07-08/M4_marking.md#L36) will be the same. The final navigation score is calculated with

~~~
nav_score = LevelProofScore + NumberOfSuccessNav * ScorePerSuccessNav - Penalty
0 ≤ nav_score ≤ 40
~~~

[NOTE] ```LevelProofScore``` is marks awarded for code or demo evidence for auto and/or waypoint navigation. You do not need to achieve a qualified run to be eligible for these marks in this demo.

##### 2.1 Semi-auto waypoint navigation (max 15pts)
- 5pts for code or demo evidence of waypoint navigation (Penalties do not apply).
- 2pts for each success navigation, maximum 10pts.
- -2pts for each marker or fruit/veg collided.
- -2pts for each time the robot crosses or goes out of the boundaries.
- Penalty limit same as M4 (4 penalties applied, 5 penalties maximum)

Final semi auto navigation score will be calculated with

~~~
nav_score = 5 + max(NumberOfSuccessNav * 2 - Penalty,0)
0 ≤ nav_score ≤ 15
~~~

##### 2.2 Full auto navigation (max 40pts)
- 5pts each for code or demo evidence of waypoint navigation and path planning (Penalties do not apply), 10pts total.
- 6pts for each success navigation, maximum 30pts.
- -3pts for each marker or fruit/veg collided
- -3pts for each time the robot crosses or goes out of the boundaries.
- Penalty limit same as M4 (4 penalties applied, 5 penalties maximum)

Final full auto navigation score will be calculated with

~~~
nav_score = 5 + 5 + max(NumberOfSuccessNav * 6 - Penalty,0)
0 ≤ nav_score ≤ 40
~~~

[NOTE] The score for each level are calculated separately and the higher of the two will be used as your final navigation score. For example, if you attempted semi-auto with 3 successful collections, 1 collisions and 1 out-of-bound, which results in ```(5) + 3*2 - 2 - 2 = 7 (pts)```. Then, you show your code evidence for full auto, which gives you ```10pts```. In that case, your navigation score will be 10pts. However, if you attempted semi-auto with 5 successful collections, 0 collisions and 0 out-of-bounds, which results in ```5 + 5*2 - 0 - 0 = 15 (pts)```. Then, you complete a fully auto run with 2 successful collections and 4 collisions, which results in ```(10) + 2*6 - 4*3 = 10 pts```. In that case, your navigation score will be 15pts. You do not need to attempt an auto level run to receive marks for path-planning evidence

---
### Rules
1. Penalty for marker/object collisions (2 pts penalty for semi auto navigation, 3 pts penalty for full auto)

2. Penalty for each time the robot goes out of bounds/touches the boundary (2 pts penalty for semi auto navigation, 3 pts penalty for full auto) (+/-1.5m from the origin, in both the x- and y-axis)

3. A maximum of 4 penalties (any of the listed above) can be applied for a run.

4. If you receive five penalties (any of the penalties listed above) during a mapping run, that run is stopped and score is calculated as is.
	
5. When your robot has stopped moving by itself during a navigation run, your navigation task score for that run will be calculated as is.
	- If you stop the run while the robot is moving, or if you manually stop the robot/script, you will receive zero navigation score for that run if the run is not [qualified](../Week07-08/M4_marking.md#L36)..

6. The **entire** robot has to stop for approximately 2 seconds within 0.5m of the target to be considered as a successful navigation to that target

7. You may skip targets.

8. If the robot reaches the targets in the wrong order, the out-of-order targets will be ignored.
    - If the shopping list specifies targets 1, 2, 3, and you reach 1, 3, 2 in that order, only 1 and 2, or 1 and 3 are counted.

9. We will review your code to see if you have implemented appropriate algorithms during the demo. To gain all marks for autonomous navigation evidence, we must find evidence of waypoint navigation and path planning in your code. Successful navigation by luck will not grant you marks

10. In any mapping or navigation attempt you must start at the origin (0, 0, 0)

11. Time limit for running your whole demo is **20min**, including mapping and navigation
	- You should end your last run ~2min before the time limit, to allow sufficient time to submit the map files or upload those files before starting the navigation task

12. The best run/attempt in each task will be considered as your final score for that task

13. You need to include everything you need for running your demo in your code submission, except for the Python venv. You are not allowed to change anything in the submission at the time of demo except for the calibrated wheel and camera parameters (baseline.txt, scale.txt, intrinsic.txt, distCoeffs.txt) and will have to run the submission as-is. You can change parameterse through command line arguments.
   
14. If you didn't submit your codes before the start of your marking lab session you will not be allowed to perform the live demo, unless special consideration has been granted.

15. You need to apply for a special consideration at least one business day before the deadline. As a group project, by default we will NOT grant a special consideration application if only one team member was unable to work on the day of the demo or days before.
    
16. If you are performing semi-automatic navigation, the waypoints you can provide are x,y coordinates, you can't specify driving instructions, such as distance / time to drive or turning angles
    
17. You will need to submit "slam.txt" and "targets.txt" files (after renaming them according to the attempt number) to the Moodle submission box during your live demo marking for arena mapping evaluation
	- The map submission must be completed within your 20min slot. If you didn't submit the maps within the allocated time you will receive 0pt for arena mapping
	- If your submitted estimation maps are not in the expected format or cannot be compared against a groundtruth map by the evaluation script you will receive 0pt for arena mapping


---
## Marking steps
**Please familiarise yourselves with these steps. This marking procedure will be used for the final demo. PLEASE MAKE SURE YOU READ THIS.**

Each team will have a VERY STRICT 20min time limit get marked, according to the [marking schedule]([https://docs.google.com/spreadsheets/d/1DcMjJF3Kf6ZLgPqqF-DbO4j7Tqobl6j5yPGE2g79kdY/edit?gid=1374030114#gid=1374030114](https://docs.google.com/spreadsheets/d/1DcMjJF3Kf6ZLgPqqF-DbO4j7Tqobl6j5yPGE2g79kdY/edit?gid=1374030114#gid=1374030114)). Note that this includes the time for you to submit your map files to Moodle. You may open up the [marking checklist](#marking-checklist), which is a simplified version of the following steps to remind yourself of the marking procedures. You MUST follow all the rules outlined in the [marking scheme](#marking-schemes), make sure you check out all the rules and understand all of them.

---
### Step 1:
**Do this BEFORE your lab session**
Zip **all required scripts** and submit your zip to the Moodle submission box. You do not need to submit your PenguinPi Python venv. Each group only needs one submission. This submission is due by the starting time of the lab session, which means you should **submit your script BEFORE you come to the lab**. You will not be allowed to perform your live demo if you didn't submit your codes on time.

**Tips:** 
- You may also include a text file in the zip file with a list of commands to use, if you don't know all the commands by heart.
- **Please practise** the marking steps (eg. unzipping your code and running it) to ensure there are no issues during marking.
- You will have to run the submission **as-is**, so make sure to double check that you submitted the right scripts. No fixes will be permitted during the time of live demo, even if it's just for renaming a file or fixing a typo.

**Important reminders:**
- The code submission is due **before the start** of your marking lab session (NOT before you run your live demo), e.g., for the Thu 3pm lab session the code submission deadline will be Thu 3pm. You will NOT be allowed to perform the live demo marking if you didn't submit your codes on time, unless a special consideration has been granted. Don't wait until the last minute and cut your submission too close.
- You will NOT be able to change the code after submission, except for the calibrated wheel and camera parameters (baseline.txt, scale.txt, intrinsic.txt, distCoeffs.txt). You will NOT be allowed to fix any typos, target label naming errors, generated maps formatting issues, indentation errors, missing files, scripts with wrong names, wrong implementation versions, wrong model weight files etc. in your submission at the time of live demo and will have to **run your downloaded code submission AS IS**.

### Step 2: 
**Do this BEFORE the demonstrator come to mark your team**
1. Connect to eduroam/internet so that you are ready to download your submission from Moodle

2. Close all the windows/applications on your Ubuntu environment

3. Use any team member's account to log in Moodle and navigate to the M5 submission box, so that you are ready to download your submitted code when the demonstrator arrives

4. Have an **empty** folder named "LiveDemo" ready at the home directory, ie. it is at ```~/LiveDemo/```. This folder should remain open at all time during marking
   
5. Activate your PenguinPi Python venv and navigate to the "LiveDemo" folder in your terminal

**Tips:** 
- You could place your calibration files on your desktop, right next to the "LiveDemo" folder, so that it will be convenient to copy those into the folder later on
- The shopping list for navigation will be posted on slack at the start of your lab session. You may also place these files on your desktop


### Step 3:
**During marking**
There two key tasks, mapping (ArUco markers + object locations) and navigation. It is up to you how you want to spend your time to do any of these tasks. 

1. Note that each lab session will use slightly different arenas

2. When the demonstrator starts to mark you, download your submitted zip file from Moodle and unzip its content to the "LiveDemo" folder. 

3. Replace the wheel and camera calibration files within the "LiveDemo" folder if needed and place the given shopping list file in the "LiveDemo" folder.

4. You may now connect to the robot attempt the mapping and navigation tasks in whatever sequence, as many times as you want within the time limit

5. You should rename the ```slam.txt``` and the ```targets.txt``` files after each attempt, please use the following naming format:
    - slam_{attempt_no}.txt, e.g. slam_run1.txt
    - targets_{attempt_no}.txt, e.g. targets_run1.txt
    - you may receive zero mapping score if you fail to follow this naming format or if your generated maps are not in the expected format (e.g., your target name labels are capitalised or has typo)

6. For semi-auto navigation, you may enter as many waypoints as you want. For full-auto navigation, you can only input a single command to start the process
    - You will receive a zero score if any additional commands are sent

#### Step 4:
**Right after marking**
1. Put all your ```slam.txt``` and ```targets.txt``` map files (after renaming them following the format in Step 3) in a folder named M5_maps (e.g. M5_maps), then zip and submit M5_maps.zip on Moodle. 

---
## Marking checklist
**BEFORE the lab session**
- [ ] Submit your code to Moodle

**BEFORE the marking**
- [ ] Close all programs and folders
- [ ] Login Moodle and navigate to the submission box
- [ ] Open an empty folder named "LiveDemo"
- [ ] Calibrate the robot if needed
- [ ] Connect your Wifi to eduroam/internet so you are ready to download the submission
- [ ] Activate Python venv if needed

**During the marking** (20min time limit)
- [ ] Shopping list for your session will be released on slack
- [ ] Demonstrator will ask you to download your submission and unzip it to "LiveDemo"
- [ ] Copy the shopping list (and the calibration files if you re-calibrated) to "LiveDemo"
- [ ] Connect to robot
- [ ] Run your M5 as many times as you want and as time permit and good luck!
- [ ] Submit your renamed ```slam.txt``` and ```targets.txt``` map files to Moodle
